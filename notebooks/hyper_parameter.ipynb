{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/hieutt/UniCon/notebooks', '/home/hieutt/miniconda3/envs/torchtf/lib/python39.zip', '/home/hieutt/miniconda3/envs/torchtf/lib/python3.9', '/home/hieutt/miniconda3/envs/torchtf/lib/python3.9/lib-dynload', '', '/home/hieutt/miniconda3/envs/torchtf/lib/python3.9/site-packages', '../']\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.functional as F\n",
    "import pprint\n",
    "from torchvision import transforms\n",
    "from dataset import CANDatasetEnet as CANDataset\n",
    "from losses import SupConLoss, UniConLoss\n",
    "from networks.efficient_net import ConEfficientNet, LinearClassifier\n",
    "from networks.efficient_net_b0 import ConEfficientNet as E_Unicon_B0, LinearClassifier as L_Unicon_B0\n",
    "from util import TwoCropTransform, AverageMeter, AddGaussianNoise\n",
    "from util import warmup_learning_rate\n",
    "from util import get_universum\n",
    "from util import save_model ,load_checkpoint, accuracy\n",
    "# from networks.classifier import LinearClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "# Định nghĩa cấu trúc `Opt` với các trường cần thiết\n",
    "Opt = namedtuple('Opt', ['lamda', 'mix', 'device', 'data_folder', 'num_workers', 'batch_size'])\n",
    "\n",
    "# Tạo đối tượng opt\n",
    "opt = Opt(\n",
    "    lamda=0.5,\n",
    "    mix='mixup',\n",
    "    device='cuda',\n",
    "    data_folder='../data/can-ml/2017-subaru-forester/preprocessed/size_64_10/TFRecord_w64_s32/2',\n",
    "    num_workers=8,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "# Truy cập các tham số\n",
    "print(opt.lamda)\n",
    "print(opt.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm này sẽ lấy một subset nhỏ từ dataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "\n",
    "def get_subset_loader(dataset, subset_ratio=0.1, batch_size=32):\n",
    "    subset_size = int(len(dataset) * subset_ratio)  # Tính số lượng phần tử của subset\n",
    "    indices = torch.randperm(len(dataset)).tolist()[:subset_size]  # Chọn ngẫu nhiên các chỉ số\n",
    "    subset = Subset(dataset, indices)  # Tạo Subset từ dataset gốc\n",
    "    \n",
    "    # Tạo DataLoader từ subset\n",
    "    subset_loader = DataLoader(subset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)  # Chỉnh batch_size, num_workers theo yêu cầu\n",
    "    return subset_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_loader(opt, batch_size, subset_ratio=0.1):\n",
    "    mean = (0.5, 0.5, 0.5)\n",
    "    std = (0.5, 0.5, 0.5)\n",
    "    normalize = transforms.Normalize(mean=mean, std=std)\n",
    "    transform = transforms.Compose([normalize])\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomApply([AddGaussianNoise(0., 0.05)], p=0.5),\n",
    "        transforms.RandomErasing(p=0.3, scale=(0.02, 0.15)),\n",
    "        transforms.Normalize(mean=mean, std=std)\n",
    "    ])\n",
    "    \n",
    "    # Khởi tạo dataset\n",
    "    train_dataset = CANDataset(root_dir=opt.data_folder, window_size=32, is_train=True, transform=TwoCropTransform(train_transform))\n",
    "    test_dataset = CANDataset(root_dir=opt.data_folder, window_size=32, is_train=False, transform=transform)\n",
    "\n",
    "    # Lấy subset nhỏ của dataset\n",
    "    train_loader = get_subset_loader(train_dataset, subset_ratio=subset_ratio, batch_size=batch_size)  \n",
    "    test_loader = get_subset_loader(test_dataset, subset_ratio=subset_ratio, batch_size=batch_size)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_predict(outputs):\n",
    "    _, pred = outputs.topk(1, 1, True, True)\n",
    "    pred = pred.t().cpu().numpy().squeeze(0)\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(train_loader, model, classifier, criterion, optimizer, epoch, opt):\n",
    "    model.eval()\n",
    "    classifier.train()\n",
    "\n",
    "    losses = AverageMeter()\n",
    "    accs = AverageMeter()\n",
    "\n",
    "    # Start the training loop\n",
    "    end = time.time()\n",
    "    for idx, (images, labels) in enumerate(train_loader):\n",
    "        images = images[0]\n",
    "        # if torch.cuda.is_available():\n",
    "        #     images = images.cuda(non_blocking=True)\n",
    "        #     labels = labels.cuda(non_blocking=True)\n",
    "        images = images.to(opt.device, non_blocking=True)\n",
    "        labels = labels.to(opt.device, non_blocking=True)\n",
    "        bsz = labels.size(0)  # Batch size\n",
    "\n",
    "        # Extract features from the pre-trained model in evaluation mode\n",
    "        with torch.no_grad():\n",
    "            features = model.encoder(images)\n",
    "\n",
    "        # Forward pass through the classifier\n",
    "        output = classifier(features.detach())\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(output, labels)\n",
    "        losses.update(loss.item(), bsz)\n",
    "\n",
    "        # Compute accuracy\n",
    "        acc = accuracy(output, labels, topk=(1,))\n",
    "        accs.update(acc[0].item(), bsz)\n",
    "\n",
    "        # Backward pass and optimization step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, classifier, criterion, opt):\n",
    "    model.eval()\n",
    "    classifier.eval()\n",
    "    \n",
    "    losses = AverageMeter()\n",
    "    total_pred = np.array([], dtype=int)\n",
    "    total_label = np.array([], dtype=int) \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader): \n",
    "            # if torch.cuda.is_available():\n",
    "            #     images = images.cuda(non_blocking=True)\n",
    "            #     labels = labels.cuda(non_blocking=True)\n",
    "            images = images.to(opt.device, non_blocking=True)\n",
    "            labels = labels.to(opt.device, non_blocking=True)\n",
    "            features = model.encoder(images)\n",
    "            outputs = classifier(features)\n",
    "\n",
    "            bsz = labels.size(0)\n",
    "            loss = criterion(outputs, labels)\n",
    "            losses.update(loss.item(), bsz)\n",
    "            \n",
    "            pred = get_predict(outputs)\n",
    "\n",
    "            if isinstance(pred, torch.Tensor):\n",
    "                pred = pred.cpu().numpy()\n",
    "            \n",
    "            if isinstance(labels, torch.Tensor):\n",
    "                labels = labels.cpu().numpy()\n",
    "\n",
    "            total_pred = np.concatenate((total_pred, pred), axis=0)\n",
    "            total_label = np.concatenate((total_label, labels), axis=0)\n",
    "    \n",
    "    acc = accuracy_score(total_label, total_pred)\n",
    "    acc = acc * 100\n",
    "    f1 = f1_score(total_label, total_pred, average='weighted')\n",
    "    \n",
    "    return acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_state_dict(state_dict):\n",
    "    \"\"\"\n",
    "    Because state dict in distributed GPU is different\n",
    "    \"\"\"\n",
    "    new_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        k = k.replace(\"module.\", \"\")\n",
    "        new_state_dict[k] = v\n",
    "    return new_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ConEfficientNet(embedding_dim=1792, feat_dim=128, head='mlp', pretrained=False)\n",
    "# classifier = LinearClassifier(input_dim=1792, num_classes=10)\n",
    "\n",
    "model = E_Unicon_B0(embedding_dim=1280, feat_dim=128, head='mlp', pretrained=False)\n",
    "classifier = L_Unicon_B0(input_dim=1280, num_classes=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save_path = '../save/CAN-ML_models/UniCon/UniCon_CAN-ML_efficient-net_lr_0.05_decay_0.0001_bsz_64_temp_0.07_mixup_lambda_0.5_trial_can_ml_con_enet_b4_64_cosine_warm'\n",
    "# ckpt_epoch = 158\n",
    "# # save_path = '../save/CAN-ML_models/UniCon/UniCon_CAN-ML_resnet50_lr_0.05_decay_0.0001_bsz_64_temp_0.07_mixup_lambda_0.5_trial_can_ml_uni_resnet_cosine_warm/'\n",
    "# # ckpt_epoch = 37\n",
    "# model_path = f'{save_path}/last.pth'\n",
    "# model_path = f'{save_path}/ckpt_epoch_{ckpt_epoch}.pth'\n",
    "# ckpt = torch.load(model_path, weights_only=False)\n",
    "# state_dict = ckpt['model']\n",
    "# state_dict = change_state_dict(state_dict)\n",
    "# model.load_state_dict(state_dict=state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "def train_encoder(train_loader, learning_rate, criterion, optimizer):\n",
    "  model.train()\n",
    "\n",
    "  for images, labels in train_loader:\n",
    "      image1, image2 = images[0], images[1]\n",
    "      images = torch.cat([image1, image2], dim=0)\n",
    "      images = images.to('cuda')\n",
    "      labels = labels.to('cuda')\n",
    "      bsz = labels.shape[0]\n",
    "\n",
    "      universum = get_universum(images, labels, opt)\n",
    "      uni_features = model(universum)\n",
    "\n",
    "      features = model(images)\n",
    "      f1, f2 = torch.split(features, [bsz, bsz], dim=0)\n",
    "      features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)\n",
    "\n",
    "      loss = criterion(features, uni_features, labels)\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def objective(trial):\n",
    "    # 1. Tối ưu các siêu tham số\n",
    "    torch.cuda.empty_cache()\n",
    "    # learning_rate = trial.suggest_float('learning_rate', 1e-5, 0.001, log=True)\n",
    "    # new_batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128, 256])\n",
    "    learning_rate = 0.05  # Ví dụ, bạn muốn dùng giá trị này cho tất cả các trial\n",
    "\n",
    "    # Sử dụng Optuna để chọn batch_size\n",
    "    batch_sizes = [8, 16, 32, 64, 128]\n",
    "    new_batch_size = batch_sizes[trial.number % len(batch_sizes)]\n",
    "\n",
    "    train_loader, val_loader = set_loader(opt, new_batch_size)\n",
    "\n",
    "    model.to(opt.device)\n",
    "    classifier.to(opt.device)\n",
    "    # model = nn.DataParallel(model, device_ids=[0, 1])  # Chạy trên 2 GPU\n",
    "    # classifier = nn.DataParallel(classifier, device_ids=[0, 1])  # Nếu classifier cũng cần\n",
    "    # 4. Loss và Optimizer (chỉ tối ưu classifier)\n",
    "    criterion = UniConLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-4)\n",
    "    criterion_classifier = torch.nn.CrossEntropyLoss()\n",
    "    optimizer_classifier = torch.optim.SGD(classifier.parameters(), lr=0.01, momentum=0.9, weight_decay=0)\n",
    "\n",
    "    step = 0\n",
    "    for epoch in range(5):\n",
    "        train_encoder(train_loader, learning_rate, criterion, optimizer)\n",
    "        train_classifier(\n",
    "            train_loader, model, classifier, criterion_classifier, optimizer_classifier, epoch, opt\n",
    "        )\n",
    "        torch.cuda.empty_cache()\n",
    "    acc, f1 = validate(\n",
    "        val_loader, model, classifier, criterion_classifier, opt\n",
    "    )\n",
    "\n",
    "    print(f\"[Trial {trial.number}] acc: {acc:.2f}, f1: {f1} lr: {learning_rate}, batch: {new_batch_size}\")\n",
    "\n",
    "    return f1  # F1 để Optuna tối ưu\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage=\"sqlite:///db.sqlite3\",  # Specify the storage URL here.\n",
    "study_name=\"hyper-parameter\",  # Unique identifier of the study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-28 09:57:46,881] A new study created in memory with name: no-name-cb78185f-b8a2-4de2-9639-607013860789\n",
      "100%|██████████| 328/328 [00:04<00:00, 72.32it/s]\n",
      "[I 2025-03-28 10:04:34,924] Trial 0 finished with value: 0.6352054339687556 and parameters: {}. Best is trial 0 with value: 0.6352054339687556.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 0] acc: 62.80, f1: 0.6352054339687556 lr: 0.05, batch: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 164/164 [00:02<00:00, 66.38it/s]\n",
      "[I 2025-03-28 10:08:09,516] Trial 1 finished with value: 0.7101599570479864 and parameters: {}. Best is trial 1 with value: 0.7101599570479864.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 1] acc: 77.38, f1: 0.7101599570479864 lr: 0.05, batch: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:01<00:00, 41.26it/s]\n",
      "[I 2025-03-28 10:10:15,543] Trial 2 finished with value: 0.7061265245998999 and parameters: {}. Best is trial 1 with value: 0.7101599570479864.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 2] acc: 79.32, f1: 0.7061265245998999 lr: 0.05, batch: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:02<00:00, 20.32it/s]\n",
      "[I 2025-03-28 10:11:49,182] Trial 3 finished with value: 0.7331108743493506 and parameters: {}. Best is trial 3 with value: 0.7331108743493506.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 3] acc: 80.54, f1: 0.7331108743493506 lr: 0.05, batch: 64\n",
      "Best trial:\n",
      "  F1: 0.7331108743493506\n",
      "  Params: {}\n"
     ]
    }
   ],
   "source": [
    "# Tạo một study và bắt đầu tối ưu hóa siêu tham số\n",
    "study = optuna.create_study(direction='maximize')\n",
    "# Chạy tối ưu hóa với 100 lần thử nghiệm\n",
    "study.optimize(objective, n_trials=4)\n",
    "# In kết quả tối ưu hóa\n",
    "print(\"Best trial:\")\n",
    "print(f\"  F1: {study.best_trial.value}\")\n",
    "print(f\"  Params: {study.best_trial.params}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchtf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
