{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/hieutt/UniCon/notebooks', '/home/hieutt/miniconda3/envs/torchtf/lib/python39.zip', '/home/hieutt/miniconda3/envs/torchtf/lib/python3.9', '/home/hieutt/miniconda3/envs/torchtf/lib/python3.9/lib-dynload', '', '/home/hieutt/miniconda3/envs/torchtf/lib/python3.9/site-packages', '../']\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks.resnet_big import ConResNet\n",
    "from networks.efficient_net import ConEfficientNet\n",
    "from networks.vision_trans import CEViT\n",
    "from networks.rec_cnn import RecCNN\n",
    "from torchsummary import summary\n",
    "from thop import profile\n",
    "import torch\n",
    "import torch.nn as nn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ConResNet()\n",
    "conenet_model = ConEfficientNet(embedding_dim=1792, feat_dim=128, head='mlp', pretrained=False)\n",
    "# conenet_model = conenet_model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "summary() got an unexpected keyword argument 'depth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconenet_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: summary() got an unexpected keyword argument 'depth'"
     ]
    }
   ],
   "source": [
    "summary(conenet_model, input_size=(3, 64, 64), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.padding.ZeroPad2d'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "#Parameters (M): 6.78\n",
      "MACs (M): 0.34\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand((1, 3, 64, 64))\n",
    "macs, params = profile(conenet_model, inputs=(x,))\n",
    "\n",
    "print(f\"#Parameters (M): {params / 1e6:.2f}\")\n",
    "print(f\"MACs (M): {macs / 1e8:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "conresnet_model = ConResNet(name='resnet50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 64, 64]           1,728\n",
      "       BatchNorm2d-2           [-1, 64, 64, 64]             128\n",
      "            Conv2d-3           [-1, 64, 64, 64]           4,096\n",
      "       BatchNorm2d-4           [-1, 64, 64, 64]             128\n",
      "            Conv2d-5           [-1, 64, 64, 64]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 64, 64]             128\n",
      "            Conv2d-7          [-1, 256, 64, 64]          16,384\n",
      "       BatchNorm2d-8          [-1, 256, 64, 64]             512\n",
      "            Conv2d-9          [-1, 256, 64, 64]          16,384\n",
      "      BatchNorm2d-10          [-1, 256, 64, 64]             512\n",
      "       Bottleneck-11          [-1, 256, 64, 64]               0\n",
      "           Conv2d-12           [-1, 64, 64, 64]          16,384\n",
      "      BatchNorm2d-13           [-1, 64, 64, 64]             128\n",
      "           Conv2d-14           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-15           [-1, 64, 64, 64]             128\n",
      "           Conv2d-16          [-1, 256, 64, 64]          16,384\n",
      "      BatchNorm2d-17          [-1, 256, 64, 64]             512\n",
      "       Bottleneck-18          [-1, 256, 64, 64]               0\n",
      "           Conv2d-19           [-1, 64, 64, 64]          16,384\n",
      "      BatchNorm2d-20           [-1, 64, 64, 64]             128\n",
      "           Conv2d-21           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-22           [-1, 64, 64, 64]             128\n",
      "           Conv2d-23          [-1, 256, 64, 64]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 64, 64]             512\n",
      "       Bottleneck-25          [-1, 256, 64, 64]               0\n",
      "           Conv2d-26          [-1, 128, 64, 64]          32,768\n",
      "      BatchNorm2d-27          [-1, 128, 64, 64]             256\n",
      "           Conv2d-28          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-29          [-1, 128, 32, 32]             256\n",
      "           Conv2d-30          [-1, 512, 32, 32]          65,536\n",
      "      BatchNorm2d-31          [-1, 512, 32, 32]           1,024\n",
      "           Conv2d-32          [-1, 512, 32, 32]         131,072\n",
      "      BatchNorm2d-33          [-1, 512, 32, 32]           1,024\n",
      "       Bottleneck-34          [-1, 512, 32, 32]               0\n",
      "           Conv2d-35          [-1, 128, 32, 32]          65,536\n",
      "      BatchNorm2d-36          [-1, 128, 32, 32]             256\n",
      "           Conv2d-37          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-38          [-1, 128, 32, 32]             256\n",
      "           Conv2d-39          [-1, 512, 32, 32]          65,536\n",
      "      BatchNorm2d-40          [-1, 512, 32, 32]           1,024\n",
      "       Bottleneck-41          [-1, 512, 32, 32]               0\n",
      "           Conv2d-42          [-1, 128, 32, 32]          65,536\n",
      "      BatchNorm2d-43          [-1, 128, 32, 32]             256\n",
      "           Conv2d-44          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-45          [-1, 128, 32, 32]             256\n",
      "           Conv2d-46          [-1, 512, 32, 32]          65,536\n",
      "      BatchNorm2d-47          [-1, 512, 32, 32]           1,024\n",
      "       Bottleneck-48          [-1, 512, 32, 32]               0\n",
      "           Conv2d-49          [-1, 128, 32, 32]          65,536\n",
      "      BatchNorm2d-50          [-1, 128, 32, 32]             256\n",
      "           Conv2d-51          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-52          [-1, 128, 32, 32]             256\n",
      "           Conv2d-53          [-1, 512, 32, 32]          65,536\n",
      "      BatchNorm2d-54          [-1, 512, 32, 32]           1,024\n",
      "       Bottleneck-55          [-1, 512, 32, 32]               0\n",
      "           Conv2d-56          [-1, 256, 32, 32]         131,072\n",
      "      BatchNorm2d-57          [-1, 256, 32, 32]             512\n",
      "           Conv2d-58          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-59          [-1, 256, 16, 16]             512\n",
      "           Conv2d-60         [-1, 1024, 16, 16]         262,144\n",
      "      BatchNorm2d-61         [-1, 1024, 16, 16]           2,048\n",
      "           Conv2d-62         [-1, 1024, 16, 16]         524,288\n",
      "      BatchNorm2d-63         [-1, 1024, 16, 16]           2,048\n",
      "       Bottleneck-64         [-1, 1024, 16, 16]               0\n",
      "           Conv2d-65          [-1, 256, 16, 16]         262,144\n",
      "      BatchNorm2d-66          [-1, 256, 16, 16]             512\n",
      "           Conv2d-67          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-68          [-1, 256, 16, 16]             512\n",
      "           Conv2d-69         [-1, 1024, 16, 16]         262,144\n",
      "      BatchNorm2d-70         [-1, 1024, 16, 16]           2,048\n",
      "       Bottleneck-71         [-1, 1024, 16, 16]               0\n",
      "           Conv2d-72          [-1, 256, 16, 16]         262,144\n",
      "      BatchNorm2d-73          [-1, 256, 16, 16]             512\n",
      "           Conv2d-74          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-75          [-1, 256, 16, 16]             512\n",
      "           Conv2d-76         [-1, 1024, 16, 16]         262,144\n",
      "      BatchNorm2d-77         [-1, 1024, 16, 16]           2,048\n",
      "       Bottleneck-78         [-1, 1024, 16, 16]               0\n",
      "           Conv2d-79          [-1, 256, 16, 16]         262,144\n",
      "      BatchNorm2d-80          [-1, 256, 16, 16]             512\n",
      "           Conv2d-81          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-82          [-1, 256, 16, 16]             512\n",
      "           Conv2d-83         [-1, 1024, 16, 16]         262,144\n",
      "      BatchNorm2d-84         [-1, 1024, 16, 16]           2,048\n",
      "       Bottleneck-85         [-1, 1024, 16, 16]               0\n",
      "           Conv2d-86          [-1, 256, 16, 16]         262,144\n",
      "      BatchNorm2d-87          [-1, 256, 16, 16]             512\n",
      "           Conv2d-88          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-89          [-1, 256, 16, 16]             512\n",
      "           Conv2d-90         [-1, 1024, 16, 16]         262,144\n",
      "      BatchNorm2d-91         [-1, 1024, 16, 16]           2,048\n",
      "       Bottleneck-92         [-1, 1024, 16, 16]               0\n",
      "           Conv2d-93          [-1, 256, 16, 16]         262,144\n",
      "      BatchNorm2d-94          [-1, 256, 16, 16]             512\n",
      "           Conv2d-95          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-96          [-1, 256, 16, 16]             512\n",
      "           Conv2d-97         [-1, 1024, 16, 16]         262,144\n",
      "      BatchNorm2d-98         [-1, 1024, 16, 16]           2,048\n",
      "       Bottleneck-99         [-1, 1024, 16, 16]               0\n",
      "          Conv2d-100          [-1, 512, 16, 16]         524,288\n",
      "     BatchNorm2d-101          [-1, 512, 16, 16]           1,024\n",
      "          Conv2d-102            [-1, 512, 8, 8]       2,359,296\n",
      "     BatchNorm2d-103            [-1, 512, 8, 8]           1,024\n",
      "          Conv2d-104           [-1, 2048, 8, 8]       1,048,576\n",
      "     BatchNorm2d-105           [-1, 2048, 8, 8]           4,096\n",
      "          Conv2d-106           [-1, 2048, 8, 8]       2,097,152\n",
      "     BatchNorm2d-107           [-1, 2048, 8, 8]           4,096\n",
      "      Bottleneck-108           [-1, 2048, 8, 8]               0\n",
      "          Conv2d-109            [-1, 512, 8, 8]       1,048,576\n",
      "     BatchNorm2d-110            [-1, 512, 8, 8]           1,024\n",
      "          Conv2d-111            [-1, 512, 8, 8]       2,359,296\n",
      "     BatchNorm2d-112            [-1, 512, 8, 8]           1,024\n",
      "          Conv2d-113           [-1, 2048, 8, 8]       1,048,576\n",
      "     BatchNorm2d-114           [-1, 2048, 8, 8]           4,096\n",
      "      Bottleneck-115           [-1, 2048, 8, 8]               0\n",
      "          Conv2d-116            [-1, 512, 8, 8]       1,048,576\n",
      "     BatchNorm2d-117            [-1, 512, 8, 8]           1,024\n",
      "          Conv2d-118            [-1, 512, 8, 8]       2,359,296\n",
      "     BatchNorm2d-119            [-1, 512, 8, 8]           1,024\n",
      "          Conv2d-120           [-1, 2048, 8, 8]       1,048,576\n",
      "     BatchNorm2d-121           [-1, 2048, 8, 8]           4,096\n",
      "      Bottleneck-122           [-1, 2048, 8, 8]               0\n",
      "AdaptiveAvgPool2d-123           [-1, 2048, 1, 1]               0\n",
      "          ResNet-124                 [-1, 2048]               0\n",
      "          Linear-125                 [-1, 2048]       4,196,352\n",
      "            ReLU-126                 [-1, 2048]               0\n",
      "          Linear-127                  [-1, 128]         262,272\n",
      "================================================================\n",
      "Total params: 27,958,976\n",
      "Trainable params: 27,958,976\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 264.56\n",
      "Params size (MB): 106.66\n",
      "Estimated Total Size (MB): 371.27\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(conresnet_model, input_size=(3, 64, 64), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "#Parameters (M): 11.50\n",
      "MACs (M): 2.23\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand((1, 3, 64, 64))\n",
    "macs, params = profile(conresnet_model, inputs=(x,))\n",
    "\n",
    "print(f\"#Parameters (M): {params / 1e6:.2f}\")\n",
    "print(f\"MACs (M): {macs / 1e9:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_model = CEViT(emb_size=256, n_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "#Parameters (M): 9.68\n",
      "MACs (M): 164.00\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand((1, 3, 64, 64))\n",
    "macs, params = profile(vit_model, inputs=(x,))\n",
    "\n",
    "print(f\"#Parameters (M): {params / 1e6:.2f}\")\n",
    "print(f\"MACs (M): {macs / 1e6:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 256, 4, 4]         196,864\n",
      "         Rearrange-2              [-1, 16, 256]               0\n",
      "    PatchEmbedding-3              [-1, 17, 256]               0\n",
      "         LayerNorm-4              [-1, 17, 256]             512\n",
      "            Linear-5              [-1, 17, 768]         197,376\n",
      "           Dropout-6            [-1, 8, 17, 17]               0\n",
      "            Linear-7              [-1, 17, 256]          65,792\n",
      "MultiHeadAttention-8              [-1, 17, 256]               0\n",
      "           Dropout-9              [-1, 17, 256]               0\n",
      "      ResidualAdd-10              [-1, 17, 256]               0\n",
      "        LayerNorm-11              [-1, 17, 256]             512\n",
      "           Linear-12             [-1, 17, 1024]         263,168\n",
      "             GELU-13             [-1, 17, 1024]               0\n",
      "          Dropout-14             [-1, 17, 1024]               0\n",
      "           Linear-15              [-1, 17, 256]         262,400\n",
      "          Dropout-16              [-1, 17, 256]               0\n",
      "      ResidualAdd-17              [-1, 17, 256]               0\n",
      "        LayerNorm-18              [-1, 17, 256]             512\n",
      "           Linear-19              [-1, 17, 768]         197,376\n",
      "          Dropout-20            [-1, 8, 17, 17]               0\n",
      "           Linear-21              [-1, 17, 256]          65,792\n",
      "MultiHeadAttention-22              [-1, 17, 256]               0\n",
      "          Dropout-23              [-1, 17, 256]               0\n",
      "      ResidualAdd-24              [-1, 17, 256]               0\n",
      "        LayerNorm-25              [-1, 17, 256]             512\n",
      "           Linear-26             [-1, 17, 1024]         263,168\n",
      "             GELU-27             [-1, 17, 1024]               0\n",
      "          Dropout-28             [-1, 17, 1024]               0\n",
      "           Linear-29              [-1, 17, 256]         262,400\n",
      "          Dropout-30              [-1, 17, 256]               0\n",
      "      ResidualAdd-31              [-1, 17, 256]               0\n",
      "        LayerNorm-32              [-1, 17, 256]             512\n",
      "           Linear-33              [-1, 17, 768]         197,376\n",
      "          Dropout-34            [-1, 8, 17, 17]               0\n",
      "           Linear-35              [-1, 17, 256]          65,792\n",
      "MultiHeadAttention-36              [-1, 17, 256]               0\n",
      "          Dropout-37              [-1, 17, 256]               0\n",
      "      ResidualAdd-38              [-1, 17, 256]               0\n",
      "        LayerNorm-39              [-1, 17, 256]             512\n",
      "           Linear-40             [-1, 17, 1024]         263,168\n",
      "             GELU-41             [-1, 17, 1024]               0\n",
      "          Dropout-42             [-1, 17, 1024]               0\n",
      "           Linear-43              [-1, 17, 256]         262,400\n",
      "          Dropout-44              [-1, 17, 256]               0\n",
      "      ResidualAdd-45              [-1, 17, 256]               0\n",
      "        LayerNorm-46              [-1, 17, 256]             512\n",
      "           Linear-47              [-1, 17, 768]         197,376\n",
      "          Dropout-48            [-1, 8, 17, 17]               0\n",
      "           Linear-49              [-1, 17, 256]          65,792\n",
      "MultiHeadAttention-50              [-1, 17, 256]               0\n",
      "          Dropout-51              [-1, 17, 256]               0\n",
      "      ResidualAdd-52              [-1, 17, 256]               0\n",
      "        LayerNorm-53              [-1, 17, 256]             512\n",
      "           Linear-54             [-1, 17, 1024]         263,168\n",
      "             GELU-55             [-1, 17, 1024]               0\n",
      "          Dropout-56             [-1, 17, 1024]               0\n",
      "           Linear-57              [-1, 17, 256]         262,400\n",
      "          Dropout-58              [-1, 17, 256]               0\n",
      "      ResidualAdd-59              [-1, 17, 256]               0\n",
      "        LayerNorm-60              [-1, 17, 256]             512\n",
      "           Linear-61              [-1, 17, 768]         197,376\n",
      "          Dropout-62            [-1, 8, 17, 17]               0\n",
      "           Linear-63              [-1, 17, 256]          65,792\n",
      "MultiHeadAttention-64              [-1, 17, 256]               0\n",
      "          Dropout-65              [-1, 17, 256]               0\n",
      "      ResidualAdd-66              [-1, 17, 256]               0\n",
      "        LayerNorm-67              [-1, 17, 256]             512\n",
      "           Linear-68             [-1, 17, 1024]         263,168\n",
      "             GELU-69             [-1, 17, 1024]               0\n",
      "          Dropout-70             [-1, 17, 1024]               0\n",
      "           Linear-71              [-1, 17, 256]         262,400\n",
      "          Dropout-72              [-1, 17, 256]               0\n",
      "      ResidualAdd-73              [-1, 17, 256]               0\n",
      "        LayerNorm-74              [-1, 17, 256]             512\n",
      "           Linear-75              [-1, 17, 768]         197,376\n",
      "          Dropout-76            [-1, 8, 17, 17]               0\n",
      "           Linear-77              [-1, 17, 256]          65,792\n",
      "MultiHeadAttention-78              [-1, 17, 256]               0\n",
      "          Dropout-79              [-1, 17, 256]               0\n",
      "      ResidualAdd-80              [-1, 17, 256]               0\n",
      "        LayerNorm-81              [-1, 17, 256]             512\n",
      "           Linear-82             [-1, 17, 1024]         263,168\n",
      "             GELU-83             [-1, 17, 1024]               0\n",
      "          Dropout-84             [-1, 17, 1024]               0\n",
      "           Linear-85              [-1, 17, 256]         262,400\n",
      "          Dropout-86              [-1, 17, 256]               0\n",
      "      ResidualAdd-87              [-1, 17, 256]               0\n",
      "        LayerNorm-88              [-1, 17, 256]             512\n",
      "           Linear-89              [-1, 17, 768]         197,376\n",
      "          Dropout-90            [-1, 8, 17, 17]               0\n",
      "           Linear-91              [-1, 17, 256]          65,792\n",
      "MultiHeadAttention-92              [-1, 17, 256]               0\n",
      "          Dropout-93              [-1, 17, 256]               0\n",
      "      ResidualAdd-94              [-1, 17, 256]               0\n",
      "        LayerNorm-95              [-1, 17, 256]             512\n",
      "           Linear-96             [-1, 17, 1024]         263,168\n",
      "             GELU-97             [-1, 17, 1024]               0\n",
      "          Dropout-98             [-1, 17, 1024]               0\n",
      "           Linear-99              [-1, 17, 256]         262,400\n",
      "         Dropout-100              [-1, 17, 256]               0\n",
      "     ResidualAdd-101              [-1, 17, 256]               0\n",
      "       LayerNorm-102              [-1, 17, 256]             512\n",
      "          Linear-103              [-1, 17, 768]         197,376\n",
      "         Dropout-104            [-1, 8, 17, 17]               0\n",
      "          Linear-105              [-1, 17, 256]          65,792\n",
      "MultiHeadAttention-106              [-1, 17, 256]               0\n",
      "         Dropout-107              [-1, 17, 256]               0\n",
      "     ResidualAdd-108              [-1, 17, 256]               0\n",
      "       LayerNorm-109              [-1, 17, 256]             512\n",
      "          Linear-110             [-1, 17, 1024]         263,168\n",
      "            GELU-111             [-1, 17, 1024]               0\n",
      "         Dropout-112             [-1, 17, 1024]               0\n",
      "          Linear-113              [-1, 17, 256]         262,400\n",
      "         Dropout-114              [-1, 17, 256]               0\n",
      "     ResidualAdd-115              [-1, 17, 256]               0\n",
      "       LayerNorm-116              [-1, 17, 256]             512\n",
      "          Linear-117              [-1, 17, 768]         197,376\n",
      "         Dropout-118            [-1, 8, 17, 17]               0\n",
      "          Linear-119              [-1, 17, 256]          65,792\n",
      "MultiHeadAttention-120              [-1, 17, 256]               0\n",
      "         Dropout-121              [-1, 17, 256]               0\n",
      "     ResidualAdd-122              [-1, 17, 256]               0\n",
      "       LayerNorm-123              [-1, 17, 256]             512\n",
      "          Linear-124             [-1, 17, 1024]         263,168\n",
      "            GELU-125             [-1, 17, 1024]               0\n",
      "         Dropout-126             [-1, 17, 1024]               0\n",
      "          Linear-127              [-1, 17, 256]         262,400\n",
      "         Dropout-128              [-1, 17, 256]               0\n",
      "     ResidualAdd-129              [-1, 17, 256]               0\n",
      "       LayerNorm-130              [-1, 17, 256]             512\n",
      "          Linear-131              [-1, 17, 768]         197,376\n",
      "         Dropout-132            [-1, 8, 17, 17]               0\n",
      "          Linear-133              [-1, 17, 256]          65,792\n",
      "MultiHeadAttention-134              [-1, 17, 256]               0\n",
      "         Dropout-135              [-1, 17, 256]               0\n",
      "     ResidualAdd-136              [-1, 17, 256]               0\n",
      "       LayerNorm-137              [-1, 17, 256]             512\n",
      "          Linear-138             [-1, 17, 1024]         263,168\n",
      "            GELU-139             [-1, 17, 1024]               0\n",
      "         Dropout-140             [-1, 17, 1024]               0\n",
      "          Linear-141              [-1, 17, 256]         262,400\n",
      "         Dropout-142              [-1, 17, 256]               0\n",
      "     ResidualAdd-143              [-1, 17, 256]               0\n",
      "       LayerNorm-144              [-1, 17, 256]             512\n",
      "          Linear-145              [-1, 17, 768]         197,376\n",
      "         Dropout-146            [-1, 8, 17, 17]               0\n",
      "          Linear-147              [-1, 17, 256]          65,792\n",
      "MultiHeadAttention-148              [-1, 17, 256]               0\n",
      "         Dropout-149              [-1, 17, 256]               0\n",
      "     ResidualAdd-150              [-1, 17, 256]               0\n",
      "       LayerNorm-151              [-1, 17, 256]             512\n",
      "          Linear-152             [-1, 17, 1024]         263,168\n",
      "            GELU-153             [-1, 17, 1024]               0\n",
      "         Dropout-154             [-1, 17, 1024]               0\n",
      "          Linear-155              [-1, 17, 256]         262,400\n",
      "         Dropout-156              [-1, 17, 256]               0\n",
      "     ResidualAdd-157              [-1, 17, 256]               0\n",
      "       LayerNorm-158              [-1, 17, 256]             512\n",
      "          Linear-159              [-1, 17, 768]         197,376\n",
      "         Dropout-160            [-1, 8, 17, 17]               0\n",
      "          Linear-161              [-1, 17, 256]          65,792\n",
      "MultiHeadAttention-162              [-1, 17, 256]               0\n",
      "         Dropout-163              [-1, 17, 256]               0\n",
      "     ResidualAdd-164              [-1, 17, 256]               0\n",
      "       LayerNorm-165              [-1, 17, 256]             512\n",
      "          Linear-166             [-1, 17, 1024]         263,168\n",
      "            GELU-167             [-1, 17, 1024]               0\n",
      "         Dropout-168             [-1, 17, 1024]               0\n",
      "          Linear-169              [-1, 17, 256]         262,400\n",
      "         Dropout-170              [-1, 17, 256]               0\n",
      "     ResidualAdd-171              [-1, 17, 256]               0\n",
      "          Reduce-172                  [-1, 256]               0\n",
      "       LayerNorm-173                  [-1, 256]             512\n",
      "          Linear-174                   [-1, 10]           2,570\n",
      "================================================================\n",
      "Total params: 9,677,066\n",
      "Trainable params: 9,677,066\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 9.87\n",
      "Params size (MB): 36.92\n",
      "Estimated Total Size (MB): 46.84\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(vit_model, input_size=(3, 64, 64), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_cnn_model = RecCNN(num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "#Parameters (M): 2.47\n",
      "MACs (M): 160.20\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand((1, 3, 64, 64))\n",
    "macs, params = profile(rec_cnn_model, inputs=(x,))\n",
    "\n",
    "print(f\"#Parameters (M): {params / 1e6:.2f}\")\n",
    "print(f\"MACs (M): {macs / 1e6:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 64, 64]           1,792\n",
      "         MaxPool2d-2           [-1, 64, 32, 32]               0\n",
      "            Conv2d-3          [-1, 128, 32, 32]          73,856\n",
      "         MaxPool2d-4          [-1, 128, 16, 16]               0\n",
      "            Conv2d-5          [-1, 256, 16, 16]         295,168\n",
      "         MaxPool2d-6            [-1, 256, 8, 8]               0\n",
      " AdaptiveAvgPool2d-7            [-1, 256, 4, 4]               0\n",
      "            Linear-8                  [-1, 512]       2,097,664\n",
      "            Linear-9                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 2,473,610\n",
      "Trainable params: 2,473,610\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 4.41\n",
      "Params size (MB): 9.44\n",
      "Estimated Total Size (MB): 13.89\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(rec_cnn_model, input_size=(3, 64, 64), device='cpu')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchtf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
